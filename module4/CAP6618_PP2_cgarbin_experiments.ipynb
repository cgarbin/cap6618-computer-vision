{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAP 6618 - Machine Learning for Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christian Garbin - experiments for programming project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: modifications done to this code assume Python 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Project 2\n",
    "See guidelines on Canvas for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure that the files below have been downloaded from https://github.com/MichalDanielDobrzanski/DeepLearningPython35 \n",
    "and saved in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load (and split) the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and inspect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "training_data = list(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# A somewhat limited, but good enough for this notebook, object inspection function\n",
    "def inspect_object(description, obj):\n",
    "    print(description)\n",
    "    t = type(obj)\n",
    "\n",
    "    if t is zip:\n",
    "        print(\"This is a zip\")\n",
    "        l = list(copy.deepcopy(obj))  # to not change the original object\n",
    "        print(\"  It has {} elements\".format(len(l)))\n",
    "        print(\"  The type of the first element is {}\". format(type(l[0])))\n",
    "        inspect_object(\"First element of the zip\", l[0])\n",
    "    elif t is list:\n",
    "        print(\"This is a list\")\n",
    "        print(\"  It has {} elements\".format(len(obj)))\n",
    "        print(\"  The type of the first element is {}\". format(type(obj[0])))\n",
    "        inspect_object(\"First element of the list\", obj[0])\n",
    "    elif t is np.ndarray:\n",
    "        print(\"This is a numpy.ndarray with shape {} = {} elements - max value {}, mean value {}\".format(\n",
    "            obj.shape, obj.size, obj.max(), obj.mean()))\n",
    "        if obj.size < 20:\n",
    "            print(obj)\n",
    "        else:\n",
    "            print(\"  Not printing the array because it's large\")\n",
    "        inspect_object(\"First element of the array\", obj[0])\n",
    "    elif t is tuple:\n",
    "        print(\"This is a tuple with {} elements\".format(len(obj)))\n",
    "        if len(obj) == 2:  # dissect the most common case\n",
    "            a, b = obj\n",
    "            inspect_object(\"\\n(1 of 2) First element of the tuple\", a)\n",
    "            inspect_object(\"\\n(2 of 2) Second element of the tuple\", b)\n",
    "        else:\n",
    "            inspect_object(\"\\nFirst element of the tuple\", obj[0])\n",
    "    else:\n",
    "        print(\"This is a {}\".format(t))\n",
    "        print(\"  Its value is {}\".format(obj))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show what we have in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data\n",
      "-----\n",
      "This is a list\n",
      "  It has 50000 elements\n",
      "  The type of the first element is <class 'tuple'>\n",
      "First element of the list\n",
      "This is a tuple with 2 elements\n",
      "\n",
      "(1 of 2) First element of the tuple\n",
      "This is a numpy.ndarray with shape (784, 1) = 784 elements - max value 0.99609375, mean value 0.13714225590229034\n",
      "  Not printing the array because it's large\n",
      "First element of the array\n",
      "This is a numpy.ndarray with shape (1,) = 1 elements - max value 0.0, mean value 0.0\n",
      "[0.]\n",
      "First element of the array\n",
      "This is a <class 'numpy.float32'>\n",
      "  Its value is 0.0\n",
      "\n",
      "(2 of 2) Second element of the tuple\n",
      "This is a numpy.ndarray with shape (10, 1) = 10 elements - max value 1.0, mean value 0.1\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "First element of the array\n",
      "This is a numpy.ndarray with shape (1,) = 1 elements - max value 0.0, mean value 0.0\n",
      "[0.]\n",
      "First element of the array\n",
      "This is a <class 'numpy.float64'>\n",
      "  Its value is 0.0\n",
      "\n",
      "\n",
      "Test data\n",
      "-----\n",
      "This is a list\n",
      "  It has 10000 elements\n",
      "  The type of the first element is <class 'tuple'>\n",
      "First element of the list\n",
      "This is a tuple with 2 elements\n",
      "\n",
      "(1 of 2) First element of the tuple\n",
      "This is a numpy.ndarray with shape (784, 1) = 784 elements - max value 0.99609375, mean value 0.09194634854793549\n",
      "  Not printing the array because it's large\n",
      "First element of the array\n",
      "This is a numpy.ndarray with shape (1,) = 1 elements - max value 0.0, mean value 0.0\n",
      "[0.]\n",
      "First element of the array\n",
      "This is a <class 'numpy.float32'>\n",
      "  Its value is 0.0\n",
      "\n",
      "(2 of 2) Second element of the tuple\n",
      "This is a <class 'numpy.int64'>\n",
      "  Its value is 7\n",
      "\n",
      "\n",
      "Validation data\n",
      "-----\n",
      "This is a list\n",
      "  It has 10000 elements\n",
      "  The type of the first element is <class 'tuple'>\n",
      "First element of the list\n",
      "This is a tuple with 2 elements\n",
      "\n",
      "(1 of 2) First element of the tuple\n",
      "This is a numpy.ndarray with shape (784, 1) = 784 elements - max value 0.99609375, mean value 0.1115373894572258\n",
      "  Not printing the array because it's large\n",
      "First element of the array\n",
      "This is a numpy.ndarray with shape (1,) = 1 elements - max value 0.0, mean value 0.0\n",
      "[0.]\n",
      "First element of the array\n",
      "This is a <class 'numpy.float32'>\n",
      "  Its value is 0.0\n",
      "\n",
      "(2 of 2) Second element of the tuple\n",
      "This is a <class 'numpy.int64'>\n",
      "  Its value is 3\n"
     ]
    }
   ],
   "source": [
    "inspect_object(\"\\nTraining data\\n-----\", training_data)\n",
    "inspect_object(\"\\n\\nTest data\\n-----\", list(test_data))\n",
    "inspect_object(\"\\n\\nValidation data\\n-----\", list(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations from the inspection:\n",
    "\n",
    "* The pixels in the image are already normalized.\n",
    "* The training labels are hot-encoded.\n",
    "* The validation and test labels are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Set up a first network with three layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer: 784 neurons<br>\n",
    "Hidden layer: 30 neurons<br>\n",
    "Output layer: 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import network\n",
    "net1 = network.Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train the network using SGD  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 epochs, mini-batch size = 10, and eta = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 8969 / 10000\n",
      "Epoch 1 : 9169 / 10000\n",
      "Epoch 2 : 9317 / 10000\n",
      "Epoch 3 : 9352 / 10000\n",
      "Epoch 4 : 9380 / 10000\n",
      "Epoch 5 : 9410 / 10000\n",
      "Epoch 6 : 9420 / 10000\n",
      "Epoch 7 : 9415 / 10000\n",
      "Epoch 8 : 9419 / 10000\n",
      "Epoch 9 : 9426 / 10000\n",
      "Epoch 10 : 9434 / 10000\n",
      "Epoch 11 : 9449 / 10000\n",
      "Epoch 12 : 9477 / 10000\n",
      "Epoch 13 : 9480 / 10000\n",
      "Epoch 14 : 9468 / 10000\n",
      "Epoch 15 : 9425 / 10000\n",
      "Epoch 16 : 9452 / 10000\n",
      "Epoch 17 : 9480 / 10000\n",
      "Epoch 18 : 9484 / 10000\n",
      "Epoch 19 : 9471 / 10000\n",
      "Epoch 20 : 9493 / 10000\n",
      "Epoch 21 : 9452 / 10000\n",
      "Epoch 22 : 9470 / 10000\n",
      "Epoch 23 : 9464 / 10000\n",
      "Epoch 24 : 9493 / 10000\n",
      "Epoch 25 : 9486 / 10000\n",
      "Epoch 26 : 9491 / 10000\n",
      "Epoch 27 : 9482 / 10000\n",
      "Epoch 28 : 9480 / 10000\n",
      "Epoch 29 : 9483 / 10000\n",
      "CPU times: user 9min 57s, sys: 1min 26s, total: 11min 23s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reload data before training because it's a zip (can't be iterated over twice)\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# Time with j % 3 in network.py (evaluate every third epoch)\n",
    "# CPU times: user 10min 10s, sys: 1min 24s, total: 11min 34s\n",
    "# Wall time: 2min 56s\n",
    "\n",
    "# Time without j % 3\n",
    "# CPU times: user 10min 35s, sys: 1min 33s, total: 12min 9s\n",
    "# Wall time: 3min 17s\n",
    "\n",
    "# Test with no test_data (no evaluation for any epoch)\n",
    "# CPU times: user 10min 26s, sys: 1min 34s, total: 12min 1s\n",
    "# Wall time: 3min 5s\n",
    "\n",
    "# Conclusion: the evaluation step is not the critical path, so leave it in\n",
    "\n",
    "epochs = 30\n",
    "mini_batch_size = 10\n",
    "eta = 3.0\n",
    "net1.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4a: Analysis and Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network performed reasonably well, so we will leave it alone for now and use it as benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Set up a second network with three layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer: 784 neurons<br>\n",
    "Hidden layer: 100 neurons<br>\n",
    "Output layer: 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import network\n",
    "net2 = network.Network([784, 100, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Train the network using SGD  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 epochs, mini-batch size = 10, and eta = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 6214 / 10000\n",
      "Epoch 1 : 7195 / 10000\n",
      "Epoch 2 : 7329 / 10000\n",
      "Epoch 3 : 7419 / 10000\n",
      "Epoch 4 : 7441 / 10000\n",
      "Epoch 5 : 7454 / 10000\n",
      "Epoch 6 : 7496 / 10000\n",
      "Epoch 7 : 7476 / 10000\n",
      "Epoch 8 : 7506 / 10000\n",
      "Epoch 9 : 8620 / 10000\n",
      "Epoch 10 : 8636 / 10000\n",
      "Epoch 11 : 8643 / 10000\n",
      "Epoch 12 : 8640 / 10000\n",
      "Epoch 13 : 8637 / 10000\n",
      "Epoch 14 : 8629 / 10000\n",
      "Epoch 15 : 8652 / 10000\n",
      "Epoch 16 : 8646 / 10000\n",
      "Epoch 17 : 8650 / 10000\n",
      "Epoch 18 : 8658 / 10000\n",
      "Epoch 19 : 8658 / 10000\n",
      "Epoch 20 : 8644 / 10000\n",
      "Epoch 21 : 8648 / 10000\n",
      "Epoch 22 : 8662 / 10000\n",
      "Epoch 23 : 8666 / 10000\n",
      "Epoch 24 : 8674 / 10000\n",
      "Epoch 25 : 8662 / 10000\n",
      "Epoch 26 : 8680 / 10000\n",
      "Epoch 27 : 8668 / 10000\n",
      "Epoch 28 : 8675 / 10000\n",
      "Epoch 29 : 8686 / 10000\n",
      "CPU times: user 18min 56s, sys: 2min 33s, total: 21min 30s\n",
      "Wall time: 5min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reload data before training because its a zip (can't be iterated over twice)\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "epochs = 30\n",
    "mini_batch_size = 10\n",
    "eta = 3.0\n",
    "net2.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6a: Analysis and Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above is significantly worse than the results reported in  Nielsen's book.\n",
    "\n",
    "> ...this improves the results to 96.59 percent. At least in this case, using more hidden neurons helps us get better results.\n",
    "\n",
    "However, the book also has a side note:\n",
    "\n",
    "> Reader feedback indicates quite some variation in results for this experiment, and some training runs give results quite a bit worse. Using the techniques introduced in chapter 3 will greatly reduce the variation in performance across different training runs for our networks.\n",
    "\n",
    "I guess I'm one of those readers...\n",
    "\n",
    "It seems that it's taking longer for this network to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While testing the network above, I noticed that its accuracy varies significantly from one run to the next.\n",
    "\n",
    "This is illustrated in the cell below, which trains the network three times (for a few epochs, to save time).\n",
    "\n",
    "Although \"illustrated\" is somewhat dangerous to use together with \"varying\". In a different run, or a different environment, the results may not be so disparate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training round #1\n",
      "Epoch 0 : 3541 / 10000\n",
      "Epoch 1 : 3446 / 10000\n",
      "Epoch 2 : 3962 / 10000\n",
      "Epoch 3 : 3588 / 10000\n",
      "Epoch 4 : 4157 / 10000\n",
      "\n",
      "Training round #2\n",
      "Epoch 0 : 7067 / 10000\n",
      "Epoch 1 : 7438 / 10000\n",
      "Epoch 2 : 7797 / 10000\n",
      "Epoch 3 : 7826 / 10000\n",
      "Epoch 4 : 7955 / 10000\n",
      "\n",
      "Training round #3\n",
      "Epoch 0 : 6888 / 10000\n",
      "Epoch 1 : 8384 / 10000\n",
      "Epoch 2 : 8481 / 10000\n",
      "Epoch 3 : 8564 / 10000\n",
      "Epoch 4 : 8722 / 10000\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "mini_batch_size = 10\n",
    "eta = 10.0\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"\\nTraining round #{}\".format(i+1))\n",
    "    training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "    n = network.Network([784, 100, 10])\n",
    "    n.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is likely caused by the weight initialization method. It uses random values:\n",
    "\n",
    "```\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "```\n",
    "\n",
    "Each run of the network starts with a significantly different set of weights.\n",
    "\n",
    "It may eventually converge to a similar accuracy, but it may need more than the number of epochs we are using here.\n",
    "\n",
    "With that in mind, the next sections conduct some experiments. Their results may be different if we perform them again, but since we are looking at the behavior, not exact numbers, they are enough for that purpose.\n",
    "\n",
    "Also noticed that adding `numpy.random.seed(42)` to make the results reproducible (or at least help a bit with that) makes a difference. Without it, the network in the next section achieves over 90% accuracy. Adding `seed()` brings its accuracy down to the numbers we see below. It is another indication that the network is sensitive to weight initialization (as the use of `numpy.randn()` shown above indicates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first experiment to try to improve it, we will attempt a higher learning rate. The idea is that it will converge faster, reaching a higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 6201 / 10000\n",
      "Epoch 1 : 8369 / 10000\n",
      "Epoch 2 : 8395 / 10000\n",
      "Epoch 3 : 8320 / 10000\n",
      "Epoch 4 : 8609 / 10000\n",
      "Epoch 5 : 8599 / 10000\n",
      "Epoch 6 : 8665 / 10000\n",
      "Epoch 7 : 8746 / 10000\n",
      "Epoch 8 : 8648 / 10000\n",
      "Epoch 9 : 8762 / 10000\n",
      "Epoch 10 : 8779 / 10000\n",
      "Epoch 11 : 8847 / 10000\n",
      "Epoch 12 : 8753 / 10000\n",
      "Epoch 13 : 8718 / 10000\n",
      "Epoch 14 : 8866 / 10000\n",
      "Epoch 15 : 8705 / 10000\n",
      "Epoch 16 : 8754 / 10000\n",
      "Epoch 17 : 8790 / 10000\n",
      "Epoch 18 : 8744 / 10000\n",
      "Epoch 19 : 8796 / 10000\n",
      "Epoch 20 : 8871 / 10000\n",
      "Epoch 21 : 8895 / 10000\n",
      "Epoch 22 : 8659 / 10000\n",
      "Epoch 23 : 8735 / 10000\n",
      "Epoch 24 : 8900 / 10000\n",
      "Epoch 25 : 8933 / 10000\n",
      "Epoch 26 : 8914 / 10000\n",
      "Epoch 27 : 9056 / 10000\n",
      "Epoch 28 : 8940 / 10000\n",
      "Epoch 29 : 8892 / 10000\n",
      "CPU times: user 18min 57s, sys: 2min 35s, total: 21min 33s\n",
      "Wall time: 5min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reload data before training because it's a zip (can't be iterated over twice)\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# IMPORTANT: even though it's the same network architecture, must create a new one. Using the object\n",
    "# from the previous test would carryover the internal variables of the  network at that point,\n",
    "# e.g. initial weight values for this test would be the trained weight values from the previous run.\n",
    "net2 = network.Network([784, 100, 10])\n",
    "\n",
    "epochs = 30\n",
    "mini_batch_size = 10\n",
    "eta = 10.0\n",
    "net2.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is now lower than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even higher learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try again with a higher rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 2111 / 10000\n",
      "Epoch 1 : 2235 / 10000\n",
      "Epoch 2 : 1964 / 10000\n",
      "Epoch 3 : 1877 / 10000\n",
      "Epoch 4 : 1677 / 10000\n",
      "Epoch 5 : 1830 / 10000\n",
      "Epoch 6 : 1765 / 10000\n",
      "Epoch 7 : 1810 / 10000\n",
      "Epoch 8 : 1763 / 10000\n",
      "Epoch 9 : 1633 / 10000\n",
      "Epoch 10 : 1747 / 10000\n",
      "Epoch 11 : 1740 / 10000\n",
      "Epoch 12 : 1879 / 10000\n",
      "Epoch 13 : 2083 / 10000\n",
      "Epoch 14 : 1988 / 10000\n",
      "Epoch 15 : 2066 / 10000\n",
      "Epoch 16 : 2077 / 10000\n",
      "Epoch 17 : 1992 / 10000\n",
      "Epoch 18 : 2052 / 10000\n",
      "Epoch 19 : 2231 / 10000\n",
      "Epoch 20 : 2264 / 10000\n",
      "Epoch 21 : 2109 / 10000\n",
      "Epoch 22 : 2101 / 10000\n",
      "Epoch 23 : 2357 / 10000\n",
      "Epoch 24 : 2218 / 10000\n",
      "Epoch 25 : 2064 / 10000\n",
      "Epoch 26 : 2273 / 10000\n",
      "Epoch 27 : 2289 / 10000\n",
      "Epoch 28 : 2189 / 10000\n",
      "Epoch 29 : 2197 / 10000\n",
      "CPU times: user 18min 52s, sys: 2min 32s, total: 21min 24s\n",
      "Wall time: 5min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reload data before training because it's a zip (can't be iterated over twice)\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# IMPORTANT: even though it's the same network architecture, must create a new one. Using the object\n",
    "# from the previous test would carryover the internal variables of the  network at that point,\n",
    "# e.g. initial weight values for this test would be the trained weight values from the previous run.\n",
    "net2 = network.Network([784, 100, 10])\n",
    "\n",
    "epochs = 30\n",
    "mini_batch_size = 10\n",
    "eta = 20.0\n",
    "net2.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a step too far. Let's try something in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back down the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower the learning rate.\n",
    "\n",
    "And this time try fewer epochs to see if it's going in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 3542 / 10000\n",
      "Epoch 1 : 4197 / 10000\n",
      "Epoch 2 : 4886 / 10000\n",
      "Epoch 3 : 4523 / 10000\n",
      "Epoch 4 : 4371 / 10000\n",
      "Epoch 5 : 4788 / 10000\n",
      "Epoch 6 : 4736 / 10000\n",
      "Epoch 7 : 4734 / 10000\n",
      "Epoch 8 : 4549 / 10000\n",
      "Epoch 9 : 4338 / 10000\n",
      "CPU times: user 6min 17s, sys: 51.1 s, total: 7min 8s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reload data before training because it's a zip (can't be iterated over twice)\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# IMPORTANT: even though it's the same network architecture, must create a new one. Using the object\n",
    "# from the previous test would carryover the internal variables of the  network at that point,\n",
    "# e.g. initial weight values for this test would be the trained weight values from the previous run.\n",
    "net2 = network.Network([784, 100, 10])\n",
    "\n",
    "epochs = 10\n",
    "mini_batch_size = 10\n",
    "eta = 15.0\n",
    "net2.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that anything above `eta = 10.0` is worse. Try a value a bit lower than that to confirm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eta=9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the experiments above, it looks the optimal learning rate is around 10.0. \n",
    "\n",
    "Investigate some values around it to confirm, starting with 9.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 6743 / 10000\n",
      "Epoch 1 : 6706 / 10000\n",
      "Epoch 2 : 6843 / 10000\n",
      "Epoch 3 : 6817 / 10000\n",
      "Epoch 4 : 7154 / 10000\n",
      "Epoch 5 : 6810 / 10000\n",
      "Epoch 6 : 6908 / 10000\n",
      "Epoch 7 : 7001 / 10000\n",
      "Epoch 8 : 7039 / 10000\n",
      "Epoch 9 : 7105 / 10000\n",
      "Epoch 10 : 7129 / 10000\n",
      "Epoch 11 : 7076 / 10000\n",
      "Epoch 12 : 7341 / 10000\n",
      "Epoch 13 : 7206 / 10000\n",
      "Epoch 14 : 7149 / 10000\n",
      "Epoch 15 : 7211 / 10000\n",
      "Epoch 16 : 7294 / 10000\n",
      "Epoch 17 : 7497 / 10000\n",
      "Epoch 18 : 7209 / 10000\n",
      "Epoch 19 : 7333 / 10000\n",
      "Epoch 20 : 7261 / 10000\n",
      "Epoch 21 : 7326 / 10000\n",
      "Epoch 22 : 7184 / 10000\n",
      "Epoch 23 : 7292 / 10000\n",
      "Epoch 24 : 7000 / 10000\n",
      "Epoch 25 : 7151 / 10000\n",
      "Epoch 26 : 7141 / 10000\n",
      "Epoch 27 : 7109 / 10000\n",
      "Epoch 28 : 7249 / 10000\n",
      "Epoch 29 : 7190 / 10000\n",
      "CPU times: user 18min 47s, sys: 2min 31s, total: 21min 19s\n",
      "Wall time: 5min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reload data before training because it's a zip (can't be iterated over twice)\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# IMPORTANT: even though it's the same network architecture, must create a new one. Using the object\n",
    "# from the previous test would carryover the internal variables of the  network at that point,\n",
    "# e.g. initial weight values for this test would be the trained weight values from the previous run.\n",
    "net2 = network.Network([784, 100, 10])\n",
    "\n",
    "epochs = 30\n",
    "mini_batch_size = 10\n",
    "eta = 9.0\n",
    "net2.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It got better. Try a lower value to see if it can get better in this direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eta=8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowering from 10.0 to 9.0 improved. Check if lowering again improves it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 7300 / 10000\n",
      "Epoch 1 : 7400 / 10000\n",
      "Epoch 2 : 7585 / 10000\n",
      "Epoch 3 : 7605 / 10000\n",
      "Epoch 4 : 7751 / 10000\n",
      "Epoch 5 : 8000 / 10000\n",
      "Epoch 6 : 8432 / 10000\n",
      "Epoch 7 : 8441 / 10000\n",
      "Epoch 8 : 8513 / 10000\n",
      "Epoch 9 : 8483 / 10000\n",
      "Epoch 10 : 8916 / 10000\n",
      "Epoch 11 : 9562 / 10000\n",
      "Epoch 12 : 9545 / 10000\n",
      "Epoch 13 : 9564 / 10000\n",
      "Epoch 14 : 9583 / 10000\n",
      "Epoch 15 : 9600 / 10000\n",
      "Epoch 16 : 9577 / 10000\n",
      "Epoch 17 : 9560 / 10000\n",
      "Epoch 18 : 9619 / 10000\n",
      "Epoch 19 : 9595 / 10000\n",
      "Epoch 20 : 9590 / 10000\n",
      "Epoch 21 : 9635 / 10000\n",
      "Epoch 22 : 9634 / 10000\n",
      "Epoch 23 : 9600 / 10000\n",
      "Epoch 24 : 9620 / 10000\n",
      "Epoch 25 : 9657 / 10000\n",
      "Epoch 26 : 9614 / 10000\n",
      "Epoch 27 : 9606 / 10000\n",
      "Epoch 28 : 9622 / 10000\n",
      "Epoch 29 : 9602 / 10000\n",
      "CPU times: user 18min 49s, sys: 2min 32s, total: 21min 21s\n",
      "Wall time: 5min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reload data before training because it's a zip (can't be iterated over twice)\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# IMPORTANT: even though it's the same network architecture, must create a new one. Using the object\n",
    "# from the previous test would carryover the internal variables of the  network at that point,\n",
    "# e.g. initial weight values for this test would be the trained weight values from the previous run.\n",
    "net2 = network.Network([784, 100, 10])\n",
    "\n",
    "epochs = 30\n",
    "mini_batch_size = 10\n",
    "eta = 8.0\n",
    "net2.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a value between `eta = 8.0` and `eta = 9.0` is the best we can do in this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Set up a third network with only two layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer: 784 neurons<br>\n",
    "Output layer: 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import network\n",
    "net3 = network.Network([784, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Train the network using SGD  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 epochs, mini-batch size = 10, and eta = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 5621 / 10000\n",
      "Epoch 1 : 5685 / 10000\n",
      "Epoch 2 : 6581 / 10000\n",
      "Epoch 3 : 6607 / 10000\n",
      "Epoch 4 : 6598 / 10000\n",
      "Epoch 5 : 6628 / 10000\n",
      "Epoch 6 : 6606 / 10000\n",
      "Epoch 7 : 6628 / 10000\n",
      "Epoch 8 : 6618 / 10000\n",
      "Epoch 9 : 6644 / 10000\n",
      "Epoch 10 : 6639 / 10000\n",
      "Epoch 11 : 6643 / 10000\n",
      "Epoch 12 : 6662 / 10000\n",
      "Epoch 13 : 6638 / 10000\n",
      "Epoch 14 : 6650 / 10000\n",
      "Epoch 15 : 6654 / 10000\n",
      "Epoch 16 : 6659 / 10000\n",
      "Epoch 17 : 6662 / 10000\n",
      "Epoch 18 : 6656 / 10000\n",
      "Epoch 19 : 6659 / 10000\n",
      "Epoch 20 : 6651 / 10000\n",
      "Epoch 21 : 6641 / 10000\n",
      "Epoch 22 : 6662 / 10000\n",
      "Epoch 23 : 6728 / 10000\n",
      "Epoch 24 : 7455 / 10000\n",
      "Epoch 25 : 7454 / 10000\n",
      "Epoch 26 : 7474 / 10000\n",
      "Epoch 27 : 7458 / 10000\n",
      "Epoch 28 : 7476 / 10000\n",
      "Epoch 29 : 7474 / 10000\n",
      "CPU times: user 1min 4s, sys: 303 ms, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reload data before training because it's a zip (can't be iterated over twice)\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "epochs = 30\n",
    "mini_batch_size = 10\n",
    "eta = 3.0\n",
    "net3.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8a: Analysis and Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the accuracy is on the low side, try a higher learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 4660 / 10000\n",
      "Epoch 1 : 4754 / 10000\n",
      "Epoch 2 : 4862 / 10000\n",
      "Epoch 3 : 4928 / 10000\n",
      "Epoch 4 : 5128 / 10000\n",
      "Epoch 5 : 5938 / 10000\n",
      "Epoch 6 : 5970 / 10000\n",
      "Epoch 7 : 5988 / 10000\n",
      "Epoch 8 : 6117 / 10000\n",
      "Epoch 9 : 6056 / 10000\n",
      "Epoch 10 : 6025 / 10000\n",
      "Epoch 11 : 6172 / 10000\n",
      "Epoch 12 : 6029 / 10000\n",
      "Epoch 13 : 6077 / 10000\n",
      "Epoch 14 : 6102 / 10000\n",
      "Epoch 15 : 6196 / 10000\n",
      "Epoch 16 : 6149 / 10000\n",
      "Epoch 17 : 6133 / 10000\n",
      "Epoch 18 : 6158 / 10000\n",
      "Epoch 19 : 6219 / 10000\n",
      "Epoch 20 : 6236 / 10000\n",
      "Epoch 21 : 6234 / 10000\n",
      "Epoch 22 : 6201 / 10000\n",
      "Epoch 23 : 6240 / 10000\n",
      "Epoch 24 : 6254 / 10000\n",
      "Epoch 25 : 6212 / 10000\n",
      "Epoch 26 : 6156 / 10000\n",
      "Epoch 27 : 6187 / 10000\n",
      "Epoch 28 : 6206 / 10000\n",
      "Epoch 29 : 6172 / 10000\n",
      "CPU times: user 1min 4s, sys: 269 ms, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reload data before training because it's a zip (can't be iterated over twice)\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# IMPORTANT: even though it's the same network architecture, must create a new one. Using the object\n",
    "# from the previous test would carryover the internal variables of the  network at that point,\n",
    "# e.g. initial weight values for this test would be the trained weight values from the previous run.\n",
    "net3 = network.Network([784, 10])\n",
    "\n",
    "epochs = 30\n",
    "mini_batch_size = 10\n",
    "eta = 12.0\n",
    "net3.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with a high learning rate the accuracy is low. It is in the same range we had when the learning rate was low.\n",
    "\n",
    "It looks like this network simply doesn't have enough representation capacity for this dataset to go beyond this range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Classification using a built-in neural network from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9a: load and inspect the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset in a sklearn-friendly format (from A. Géron's book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Geron \n",
    "\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    # No need to save pics\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_target(mnist):\n",
    "    reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]\n",
    "    reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]\n",
    "    mnist.data[:60000] = mnist.data[reorder_train]\n",
    "    mnist.target[:60000] = mnist.target[reorder_train]\n",
    "    mnist.data[60000:] = mnist.data[reorder_test + 60000]\n",
    "    mnist.target[60000:] = mnist.target[reorder_test + 60000]\n",
    "\n",
    "try:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "    mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings\n",
    "    sort_by_target(mnist) # fetch_openml() returns an unsorted dataset\n",
    "except ImportError:\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and test data and shuffle the training data to help classifiers that can get stuck a local minimum.\n",
    "\n",
    "**IMPORTANT**: In the code below we are using the first 50,000 elements as the train set to match Nielsen's code. If we train with all 60,000 elements we would be giving this classifier an unfair edge over Nielsen's classifiers tested above.\n",
    "\n",
    "Before we slice the 50,000 elements we need to shuffle the complete train set we get from MNIST. Otherwise we will not be sure we are getting the correct distribution for all digits (shuffling doesn't guarantee that either - we will check with a histogram below).\n",
    "\n",
    "The test data is still the last 10,000 elements, as defined in the MNIST dataset. We currently don't use the elements in the [50,001-60,000] range. They would be used as a validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# Shuffle the complete train set\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "# Now extract the first 50,000\n",
    "X_train, y_train = X_train[:50000], y_train[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the dataset contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train\n",
      "-----\n",
      "This is a numpy.ndarray with shape (50000, 784) = 39200000 elements - max value 255.0, mean value 33.29885793367347\n",
      "  Not printing the array because it's large\n",
      "First element of the array\n",
      "This is a numpy.ndarray with shape (784,) = 784 elements - max value 255.0, mean value 20.427295918367346\n",
      "  Not printing the array because it's large\n",
      "First element of the array\n",
      "This is a <class 'numpy.float64'>\n",
      "  Its value is 0.0\n",
      "\n",
      "\n",
      "y_train\n",
      "-----\n",
      "This is a numpy.ndarray with shape (50000,) = 50000 elements - max value 9, mean value 4.46016\n",
      "  Not printing the array because it's large\n",
      "First element of the array\n",
      "This is a <class 'numpy.int8'>\n",
      "  Its value is 1\n",
      "\n",
      "\n",
      "X_text\n",
      "-----\n",
      "This is a numpy.ndarray with shape (10000, 784) = 7840000 elements - max value 255.0, mean value 33.791224489795916\n",
      "  Not printing the array because it's large\n",
      "First element of the array\n",
      "This is a numpy.ndarray with shape (784,) = 784 elements - max value 255.0, mean value 47.21173469387755\n",
      "  Not printing the array because it's large\n",
      "First element of the array\n",
      "This is a <class 'numpy.float64'>\n",
      "  Its value is 0.0\n",
      "\n",
      "\n",
      "y_test\n",
      "-----\n",
      "This is a numpy.ndarray with shape (10000,) = 10000 elements - max value 9, mean value 4.4434\n",
      "  Not printing the array because it's large\n",
      "First element of the array\n",
      "This is a <class 'numpy.int8'>\n",
      "  Its value is 0\n"
     ]
    }
   ],
   "source": [
    "inspect_object(\"\\nX_train\\n-----\", X_train)\n",
    "inspect_object(\"\\n\\ny_train\\n-----\", y_train)\n",
    "inspect_object(\"\\n\\nX_text\\n-----\", X_test)\n",
    "inspect_object(\"\\n\\ny_test\\n-----\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we have a good distribution for all digits.\n",
    "\n",
    "What we are looking for: about the same distribution of digits in the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(121); plt.hist(y_train); plt.ylabel('Train set - number of digits in each class')\n",
    "plt.subplot(122); plt.hist(y_test); plt.ylabel('Test set - number of digits in each class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9b: 30 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.99334600\n",
      "Iteration 2, loss = 0.63490822\n",
      "Iteration 3, loss = 0.59732142\n",
      "Iteration 4, loss = 0.58117939\n",
      "Iteration 5, loss = 0.53209930\n",
      "Iteration 6, loss = 0.50404391\n",
      "Iteration 7, loss = 0.51194670\n",
      "Iteration 8, loss = 0.51191745\n",
      "Iteration 9, loss = 0.49157505\n",
      "Iteration 10, loss = 0.48038426\n",
      "Iteration 11, loss = 0.47626838\n",
      "Iteration 12, loss = 0.48514311\n",
      "Iteration 13, loss = 0.50707610\n",
      "Iteration 14, loss = 0.50824714\n",
      "Iteration 15, loss = 0.48658379\n",
      "Iteration 16, loss = 0.47780768\n",
      "Iteration 17, loss = 0.48340429\n",
      "Iteration 18, loss = 0.45106580\n",
      "Iteration 19, loss = 0.43135839\n",
      "Iteration 20, loss = 0.45545184\n",
      "Iteration 21, loss = 0.45065506\n",
      "Iteration 22, loss = 0.46412175\n",
      "Iteration 23, loss = 0.43301812\n",
      "Iteration 24, loss = 0.43914636\n",
      "Iteration 25, loss = 0.42105670\n",
      "Iteration 26, loss = 0.42633173\n",
      "Iteration 27, loss = 0.40124111\n",
      "Iteration 28, loss = 0.45041884\n",
      "Iteration 29, loss = 0.42703379\n",
      "Iteration 30, loss = 0.41126832\n",
      "MLPClassifier(activation='logistic', alpha=0, batch_size=10, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(30,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=30, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "CPU times: user 5min 37s, sys: 45.4 s, total: 6min 22s\n",
      "Wall time: 1min 39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cgarbin/fau/cap6618/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(30,), activation=\"logistic\", solver=\"sgd\", alpha=0, batch_size=10, max_iter=30, verbose=True)\n",
    "fit_results = clf.fit(X_train, y_train)\n",
    "\n",
    "# Check:\n",
    "#  Turn off momentum\n",
    "\n",
    "print(fit_results)  # must explicitly print because of %%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8821"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate accuracy on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9c: 100 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68428837\n",
      "Iteration 2, loss = 0.46354329\n",
      "Iteration 3, loss = 0.43010842\n",
      "Iteration 4, loss = 0.42461680\n",
      "Iteration 5, loss = 0.40872894\n",
      "Iteration 6, loss = 0.40468090\n",
      "Iteration 7, loss = 0.38880387\n",
      "Iteration 8, loss = 0.37933068\n",
      "Iteration 9, loss = 0.37007991\n",
      "Iteration 10, loss = 0.36352360\n",
      "Iteration 11, loss = 0.36126393\n",
      "Iteration 12, loss = 0.37309771\n",
      "Iteration 13, loss = 0.35262089\n",
      "Iteration 14, loss = 0.35409925\n",
      "Iteration 15, loss = 0.35294832\n",
      "Iteration 16, loss = 0.33653540\n",
      "Iteration 17, loss = 0.32901280\n",
      "Iteration 18, loss = 0.31764981\n",
      "Iteration 19, loss = 0.31136596\n",
      "Iteration 20, loss = 0.31133030\n",
      "Iteration 21, loss = 0.31592339\n",
      "Iteration 22, loss = 0.30800188\n",
      "Iteration 23, loss = 0.30701879\n",
      "Iteration 24, loss = 0.30570779\n",
      "Iteration 25, loss = 0.30434285\n",
      "Iteration 26, loss = 0.29505947\n",
      "Iteration 27, loss = 0.29182182\n",
      "Iteration 28, loss = 0.29142902\n",
      "Iteration 29, loss = 0.28933034\n",
      "Iteration 30, loss = 0.29483133\n",
      "MLPClassifier(activation='logistic', alpha=0, batch_size=10, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=30, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "CPU times: user 14min 24s, sys: 1min 42s, total: 16min 7s\n",
      "Wall time: 4min 6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cgarbin/fau/cap6618/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation=\"logistic\", solver=\"sgd\", alpha=0, batch_size=10, max_iter=30, verbose=True)\n",
    "fit_results = clf.fit(X_train, y_train)\n",
    "\n",
    "# Check:\n",
    "#  Turn off momentum\n",
    "\n",
    "print(fit_results)  # must explicitly print because of %%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9093"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate accuracy on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9d: Scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nielsen's MNIST dataset is scaled. Check if this classifier improves if we also scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66192274\n",
      "Iteration 2, loss = 0.30563759\n",
      "Iteration 3, loss = 0.25847513\n",
      "Iteration 4, loss = 0.23202599\n",
      "Iteration 5, loss = 0.21286834\n",
      "Iteration 6, loss = 0.19740597\n",
      "Iteration 7, loss = 0.18467613\n",
      "Iteration 8, loss = 0.17338370\n",
      "Iteration 9, loss = 0.16355954\n",
      "Iteration 10, loss = 0.15470910\n",
      "Iteration 11, loss = 0.14671616\n",
      "Iteration 12, loss = 0.13937744\n",
      "Iteration 13, loss = 0.13265760\n",
      "Iteration 14, loss = 0.12647840\n",
      "Iteration 15, loss = 0.12087223\n",
      "Iteration 16, loss = 0.11555759\n",
      "Iteration 17, loss = 0.11052733\n",
      "Iteration 18, loss = 0.10595033\n",
      "Iteration 19, loss = 0.10174277\n",
      "Iteration 20, loss = 0.09757501\n",
      "Iteration 21, loss = 0.09380729\n",
      "Iteration 22, loss = 0.09018419\n",
      "Iteration 23, loss = 0.08670448\n",
      "Iteration 24, loss = 0.08350993\n",
      "Iteration 25, loss = 0.08042404\n",
      "Iteration 26, loss = 0.07751407\n",
      "Iteration 27, loss = 0.07483662\n",
      "Iteration 28, loss = 0.07212204\n",
      "Iteration 29, loss = 0.06960266\n",
      "Iteration 30, loss = 0.06714081\n",
      "MLPClassifier(activation='logistic', alpha=0, batch_size=10, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=30, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "CPU times: user 8min 2s, sys: 56.6 s, total: 8min 58s\n",
      "Wall time: 2min 19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cgarbin/fau/cap6618/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation=\"logistic\", solver=\"sgd\", alpha=0, batch_size=10, max_iter=30, verbose=True)\n",
    "fit_results = clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(fit_results)  # must explicitly print because of %%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8858"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate accuracy on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It decreased, but not by much. At this scale, it can be just random values changing it in one direction or another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9e: without momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MLPClassifier` uses momentum of 0.9 by default. Nielsen's classifier doens't have momentum.\n",
    "\n",
    "This test turns off momentum to make a closer comparison between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.98516463\n",
      "Iteration 2, loss = 0.48908903\n",
      "Iteration 3, loss = 0.38378049\n",
      "Iteration 4, loss = 0.33238902\n",
      "Iteration 5, loss = 0.30097320\n",
      "Iteration 6, loss = 0.27923571\n",
      "Iteration 7, loss = 0.26258342\n",
      "Iteration 8, loss = 0.24930881\n",
      "Iteration 9, loss = 0.23874333\n",
      "Iteration 10, loss = 0.22858407\n",
      "Iteration 11, loss = 0.22100324\n",
      "Iteration 12, loss = 0.21300596\n",
      "Iteration 13, loss = 0.20839692\n",
      "Iteration 14, loss = 0.19854166\n",
      "Iteration 15, loss = 0.19286079\n",
      "Iteration 16, loss = 0.19027903\n",
      "Iteration 17, loss = 0.18072812\n",
      "Iteration 18, loss = 0.17741561\n",
      "Iteration 19, loss = 0.17358059\n",
      "Iteration 20, loss = 0.16976155\n",
      "Iteration 21, loss = 0.16500668\n",
      "Iteration 22, loss = 0.16047686\n",
      "Iteration 23, loss = 0.16032744\n",
      "Iteration 24, loss = 0.15656825\n",
      "Iteration 25, loss = 0.15232610\n",
      "Iteration 26, loss = 0.15268370\n",
      "Iteration 27, loss = 0.14742717\n",
      "Iteration 28, loss = 0.14507266\n",
      "Iteration 29, loss = 0.14172823\n",
      "Iteration 30, loss = 0.13748990\n",
      "MLPClassifier(activation='logistic', alpha=0, batch_size=10, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=30, momentum=0.0,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "CPU times: user 8min 22s, sys: 1min 5s, total: 9min 27s\n",
      "Wall time: 2min 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cgarbin/fau/cap6618/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation=\"logistic\", solver=\"sgd\", alpha=0, batch_size=10, max_iter=30, verbose=True, momentum=0.0)\n",
    "fit_results = clf.fit(X_train, y_train)\n",
    "\n",
    "print(fit_results)  # must explicitly print because of %%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9538"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate accuracy on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGD optimizer performed better without momentum. This is somewhat surprising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9f: \"Modernize\" the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use parameters that are closer to the ones used in papers and other examples:\n",
    "    \n",
    "- Adaptive optimizer\n",
    "- Regularization\n",
    "- Larger batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.16646348\n",
      "Iteration 2, loss = 0.69386058\n",
      "Iteration 3, loss = 0.38114340\n",
      "Iteration 4, loss = 0.27245863\n",
      "Iteration 5, loss = 0.20372771\n",
      "Iteration 6, loss = 0.17981297\n",
      "Iteration 7, loss = 0.15992317\n",
      "Iteration 8, loss = 0.15180641\n",
      "Iteration 9, loss = 0.14226384\n",
      "Iteration 10, loss = 0.14647421\n",
      "Iteration 11, loss = 0.12725033\n",
      "Iteration 12, loss = 0.12093640\n",
      "Iteration 13, loss = 0.11944512\n",
      "Iteration 14, loss = 0.11761806\n",
      "Iteration 15, loss = 0.10751363\n",
      "Iteration 16, loss = 0.11283352\n",
      "Iteration 17, loss = 0.10394767\n",
      "Iteration 18, loss = 0.10054191\n",
      "Iteration 19, loss = 0.09790705\n",
      "Iteration 20, loss = 0.09041647\n",
      "Iteration 21, loss = 0.09431209\n",
      "Iteration 22, loss = 0.08762820\n",
      "Iteration 23, loss = 0.09204506\n",
      "Iteration 24, loss = 0.09433479\n",
      "Iteration 25, loss = 0.08689661\n",
      "Iteration 26, loss = 0.08218786\n",
      "Iteration 27, loss = 0.07104987\n",
      "Iteration 28, loss = 0.07466367\n",
      "Iteration 29, loss = 0.08835807\n",
      "Iteration 30, loss = 0.07603521\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=30, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "CPU times: user 2min 1s, sys: 17.4 s, total: 2min 19s\n",
      "Wall time: 38.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cgarbin/fau/cap6618/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation=\"relu\", solver=\"adam\", alpha=0.0001, batch_size=100, max_iter=30, verbose=True)\n",
    "fit_results = clf.fit(X_train, y_train)\n",
    "\n",
    "print(fit_results)  # must explicitly print because of %%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9635"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate accuracy on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better, but also significantly faster to train. We could increase the number epochs and try again, but we will stop here. The goal was to identify a trend at this point, not to optimize to its best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick pass on the confusion matrix to check if there is a digit that stands out (there is not, a few stragglers, but not to a signficant degree).\n",
    "\n",
    "Confusion matrix reminder: row = actual class, column = predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 965    1    1    1    2    1    3    0    5    1]\n",
      " [   0 1122    4    0    0    1    1    2    4    1]\n",
      " [   7    1 1003    6    3    0    1    5    6    0]\n",
      " [   0    2   14  975    0   10    0    0    7    2]\n",
      " [   3    1    3    0  942    0    5    2    4   22]\n",
      " [   7    1    3   19    0  833    7    2   12    8]\n",
      " [   5    3    4    1   10    7  918    1    9    0]\n",
      " [   2    2   15   13    3    1    0  972    7   13]\n",
      " [   8    2    5    5    5    3    2    3  937    4]\n",
      " [   6    3    2   10    7    1    0    2   10  968]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the mistakes - ligher color = higher error rate.\n",
    "\n",
    "Highest error rates:\n",
    "\n",
    "- 4's mistaken as 9\n",
    "- 5's mistaken as 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize each row so one class doesn't dominate the values, skewing the heatmap\n",
    "# This is not strictly necessary in this case because MNIST classes are well-balanced\n",
    "# It's good practice nevertheless\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "conf_mx_misclassifications = conf_mx / row_sums\n",
    "\n",
    "# Remove correct classifications (leaves only misclassifications in the matrix)\n",
    "np.fill_diagonal(conf_mx_misclassifications, 0)  \n",
    "\n",
    "plt.matshow(conf_mx_misclassifications, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Build your best solution (and explain each step) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a separate notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
